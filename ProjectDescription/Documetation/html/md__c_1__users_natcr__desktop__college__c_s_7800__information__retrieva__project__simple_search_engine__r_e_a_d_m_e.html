<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.15"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Basic Search Engine: Information Retrieval: Simple Search Engine</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Basic Search Engine
   </div>
   <div id="projectbrief">For the purpose of learning we are creating a basic search engine using inverted index</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.15 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Information Retrieval: Simple Search Engine </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The primary purpose of this assignment is to get familiar with the basic term generation, indexing, and query processing algorithms and use them to implement a simple search engine.</p>
<h2>Getting Started and Prerequisites</h2>
<p>You will build your search engine with the given skeleton code in Python and test on the Cranfield Dataset <a href="http://web1.cs.wright.edu/~tkprasad/courses/cs7800/asg-S16/CranfieldDataset.zip">http://web1.cs.wright.edu/~tkprasad/courses/cs7800/asg-S16/CranfieldDataset.zip</a>. You can work individually or in a two-person team.</p>
<h3>*Setup:*</h3>
<ul>
<li>You will need to use the NLTK toolkit <a href="http://www.nltk.org/">http://www.nltk.org/</a> for stemming. For Ubuntu, it is easy to install ``` sudo pip install -U nltk ``` Install the python pip package first if you have not yet. For other platforms, please check the web site for instructions.</li>
<li>Download the Cranfield dataset <a href="http://web1.cs.wright.edu/~tkprasad/courses/cs7800/asg-S16/CranfieldDataset.zip">http://web1.cs.wright.edu/~tkprasad/courses/cs7800/asg-S16/CranfieldDataset.zip</a>. It contains three files: cran.all - the document collection, query.text - the sample queries, qrels.text - the relevant query-document pairs. Check README.txt for more details.</li>
<li>Download the skeleton code <a href="http://cecs.wright.edu/~keke.chen/ir/2019sp/prj1/prj1.zip">http://cecs.wright.edu/~keke.chen/ir/2019sp/prj1/prj1.zip</a>. You should use the skeleton code for easier grading.</li>
</ul>
<p>Project is running Python 3.7</p>
<h2>Part 1: Building Inverted Index</h2>
<p>The preprocessing steps include </p><div class="fragment"><div class="line">* split a document to a list of tokens and lowercase the tokens.</div><div class="line">* remove the stopwords. A list of stopwords has been provided - check</div><div class="line">  the file &quot;stopwords&quot; in the directory.</div><div class="line">* stemming. Use a stemmer in NLTK &lt;http://www.nltk.org/howto/stem.html&gt;.</div><div class="line">* Build mechanism for query authors, titles, and other such things.</div></div><!-- fragment --><p> Easiest way to do this is using Extent Index. <img src="images/ideaOfExentsIndex.JPG" width="500" alt="" class="inline"/>
<br />
</p>
<p>These steps are shared by both indexing and query processing. Thus, it's better to put these common functions in one place. Check the util.py file. The Cranfield document file has a special format. cran.py has been provided to simplify your work in reading the documents.</p>
<p>Once you get a list of terms, use the steps to put the terms in the inverted index and sort the lists correspondingly. Check the files: doc.py and index.py to understand how the classes are organized, and then implement the necessary methods - you are free to add more methods if necessary.</p>
<p>Finally, the index should be saved to a file. You can use any serialization method you like (e.g., JSON or any Python built-in libraries). Make sure the index can be saved/loaded correctly.</p>
<p>You should provide the executable indexing program that can be run as follows</p>
<div class="fragment"><div class="line">python index.py cran.all index_file</div></div><!-- fragment --><p>It builds the index for the cran.all file and saves the index into the index_file.</p>
<img src="images/indexing.JPG" width="500" alt="" class="inline"/>
<br />
</p>
<h2>Part 2: Query Processing</h2>
<p>The query processing component contains the following steps.</p>
<ul>
<li>Query preprocessing, which includes all the steps used for preprocessing documents in indexing. In addition, you should also use the spelling corrector. Please check the Norvig's implementation <a href="http://norvig.com/spell-correct.html">http://norvig.com/spell-correct.html</a> and understand the algorithm. The python code of the spelling corrector has been included in the skeleton code: norvig_spell.py (need some testing to make sure it work as expected).</li>
<li>Process queries with the Boolean model. Check the textbook or slides for the processing algorithm.</li>
<li>Process queries with the vector model. Use the standard TFIDF to represent the weights in the vector representation. Use the cosine similarity for ranking.</li>
<li>Query extended index</li>
</ul>
<p>If the data set contains any queries that require us to use author or other significant fields we have to use the extended indexing. <br />
 <img src="images/usingExtents.JPG" width="500" alt="" class="inline"/>
<br />
</p>
<p>Your standard query processing program should be run as follows</p>
<div class="fragment"><div class="line">python query.py index_file model_selection query.text query_id</div></div><!-- fragment --><p>where mode_selection has: 0 - Boolean, 1 - vector, 2 - batch evaluation, query.text contains the sample queries (included in the Cranfield dataset), and in mode 0 or 1 qid_or_n is the specific query_id you choose and in mode 2 qid_or_n represent randomly selecting n queries for batch evaluation. cranqry.py has been provided for reading the special format used by query.text. In mode=0 or 1 The output will be a list of document IDs for the Boolean model, and the top 3 ranked results for the vector model. For vector model, choose one of the TFIDF scoring methods, e.g., lnc.ltc, mentioned in Figure 6.15 at the page 118 of the textbook (or the same Figure in slides "scoring_idf.ppt").</p>
<p>For mode=2, it will randomly select n queries, e.g., n=20, process them, and evaluate the total time spent on processing the queries for each model (Boolean and vector) - do not print out query results in processing queries, which will pollute the evaluation of processing time. You should repeat this experiment (mode=2) for 5 times and report the result in a table (or figure).</p>
<h2>Part 3: Evaluation</h2>
<p>Check qrels.text in the Cranfield dataset for evaluating the quality of search results. You can use the ndcg_score function in metrics.py to evaluate the quality. Please finish the program "batch_eval.py" to compute the average NDCGs for the boolean-model and vector-model based query processing results, respectively, and use t-test <a href="https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_ind.html">https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_ind.html</a> or wilcoxon-test <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.wilcoxon.html">https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.wilcoxon.html</a> to get the p-value for comparison (i.e., whether one ranking result is /statistically-significantly/ better than the other).</p>
<div class="fragment"><div class="line">python batch_eval.py index_file query.text qrels.text n</div></div><!-- fragment --><p>where n is the number of randomly selected queries from query.text. The output should be the average NDCGs (for n queries) for the boolean model and the vector model respectively, and the p-value for the comparison. Repeat your evaluation for 5 times with n=50. Report the result in a table.</p>
<img src="images/NDCG.JPG" width="500" alt="" class="inline"/>
 <br />
</p>
<h3>Extra Credit</h3>
<p>(Extra credit (5%)) You can extend this query evaluation program to compare different settings of vector models (i.e., TFIDF scoring methods mentioned in Part 2): implement some different query processing methods. Then compare and report the quality of ranking result</p>
<p>Since the final exam is kind of written it is hard to determine the exact grade that will be given. Therefore 5% padding seems worth it. </p><div class="fragment"><div class="line">We can u se the below formula to do the query for TF-IDF</div></div><!-- fragment --><img src="images/TFIDF_for_Query.GIF" width="500" alt="" class="inline"/>
<br />
</p>
<p>Please test/debug all your programs thoroughly. By designing tests, ask yourself questions like</p>
<ul>
<li>Are the stopwords really removed? Are the terms in index all stemmed?</li>
<li>What is the number of terms in the dictionary and what is the size of postings? Do they make sense?</li>
<li>Are index saving and loading working as expected?</li>
<li>Do you also convert queries to terms?</li>
<li>How do you confirm that TFIDF values are computed correctly?</li>
<li>How do you confirm cosine similarity is computed correctly?</li>
<li>Are your selected sample queries getting the same results as you expect (manually computed) for boolean model processing?</li>
<li>Are your selected sample queries getting the same results as you expect for vector model processing?</li>
<li>Do the NDCGs for selected sample queries match your manually computed results?</li>
<li>etc.</li>
</ul>
<h2>Deliverables</h2>
<p>Turn in three files (DO NOT zip them into one zip file) to Pilot: (1) a brief documentation about your design, implementation, and tests you have done, (2) paste your well-commented source code in one PDF file, and (3) the zipped whole source code directory. You should put your team members names in the beginning of each PDF file.</p>
<p>Plagiarism will get 0 directly. No deadline extension will be given. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.15
</small></address>
</body>
</html>
